AWSTemplateFormatVersion: 2010-09-09

Description: The Airflow cluster stack based on Airflow Turbine Stack - modified

Parameters:

  ProjectNamePrefix:
    Description: Project Name to be associated with respective resources.
    Type: String
    Default: eksairflow-staging

  CodeDeployBucket:
    Description: Name of the S3 bucket containing the CodeDeploy packages
    Type: String

  CfnLogsBucket:
    Description: Name of the S3 bucket containing the CodeDeploy packages
    Type: String

  VpcBaseCidr:
    Type: String

  TargetVpc:
    Type: String


  # -------------------------- original----------------------------------

  DummySubnetBlock:
    Description: The IPv4 CIDR block to be used in the dummy subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.99.0/24

  StackSubnetBlock:
    Description: The IPv4 CIDR block to be used in the stack subnet.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 10.0.0.0/24

  AllowedSshBlock:
    Description: >-
      The IPv4 CIDR block to allow SSH access in all machines. The default of
      0.0.0.0/0 allows SSH from everywhere, which is convenient but less secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 0.0.0.0/0
  AllowedWebBlock:
    Description: >-
      The IPv4 CIDR block to allow HTTP access in the webserver. The default of
      0.0.0.0/0 allows HTTP from everywhere, which is convenient but less
      secure.
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
    Default: 0.0.0.0/0
  WebserverPort:
    Description: >-
      The port Airflow webserver will be listening. Ports below 1024 can be
      opened only with root privileges and the airflow process does not run as
      such.
    Type: String
    Default: 8080
  KeyPair:
    Description: Amazon EC2 Key Pair to be used paired with the EC2 instances.
    Type: 'AWS::EC2::KeyPair::KeyName'
  SchedulerInstanceType:
    Description: EC2 instance type to use for the scheduler.
    Type: String
    Default: t3.micro
  WebserverInstanceType:
    Description: EC2 instance type to use for the webserver.
    Type: String
    Default: t3.micro
  WorkerInstanceType:
    Description: EC2 instance type to use for the workers.
    Type: String
    Default: t3.small
  WorkerConcurrency:
    Description: Number of concurrent units per machine.
    Type: Number
    Default: 1
  MinGroupSize:
    Description: The minimum number of active worker instances.
    Type: Number
    Default: 0
  MaxGroupSize:
    Description: The maximum number of active worker instances.
    Type: Number
    Default: 10
  ShrinkThreshold:
    Description: >-
      The timeout (in seconds, multiple of 60) after which the queue staying
      empty will trigger the AutoScaling group to Scale In, deallocating one
      worker instance.
    Type: Number
    Default: 0.5
  GrowthThreshold:
    Description: >-
      The threshold for the average queue size from which going equal or above
      will trigger the AutoScaling group to Scale Out, allocating one worker
      instance.
    Type: Number
    Default: 0.9
  DbMasterUsername:
    Description: The username to be used in the airflow database.
    Type: String
    Default: airflow
  DbMasterPassword:
    Description: The password to be used in the airflow database.
    Type: String
    NoEcho: true
  LoadExampleDags:
    Description: >-
      Load the example DAGs distributed with Airflow. Useful if deploying a
      stack for demonstrating a few topologies, operators and scheduling
      strategies.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'
  LoadDefaultConns:
    Description: >-
      Load the default connections initialized by Airflow. Most consider these
      unnecessary, which is why the default is to not load them.
    Type: String
    AllowedValues:
      - 'False'
      - 'True'
    Default: 'False'

Mappings:
  RegionToAMI:
    ap-northeast-1:
      HVMGP2: ami-00f9d04b3b3092052
    ap-northeast-2:
      HVMGP2: ami-0c764df09c35858b8
    ap-northeast-3:
      HVMGP2: ami-0f4865fb90390aeec
    ap-south-1:
      HVMGP2: ami-00796998f258969fd
    ap-southeast-1:
      HVMGP2: ami-085fd1bd447be68e8
    ap-southeast-2:
      HVMGP2: ami-0b8dea0e70b969adc
    ca-central-1:
      HVMGP2: ami-05cac140c6a1fb960
    cn-north-1:
      HVMGP2: ami-0bcb2d94a820e559c
    cn-northwest-1:
      HVMGP2: ami-0d82dd087e5eea5af
    eu-central-1:
      HVMGP2: ami-02ea8f348fa28c108
    eu-west-1:
      HVMGP2: ami-0a5e707736615003c
    eu-west-2:
      HVMGP2: ami-017b0e29fac27906b
    eu-west-3:
      HVMGP2: ami-04992646d54c69ef4
    sa-east-1:
      HVMGP2: ami-0160a8b6087883cb6
    us-east-1:
      HVMGP2: ami-013be31976ca2c322
    us-east-2:
      HVMGP2: ami-0350c5670171b5391
    us-gov-west-1:
      HVMGP2: ami-d263f9b3
    us-west-1:
      HVMGP2: ami-01beb64058d271bc4
    us-west-2:
      HVMGP2: ami-061e7ebbc234015fe

Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          default: Networking
        Parameters:
          - VpcBaseCidr
          - DummySubnetBlock
          - StackSubnetBlock
          - AllowedSshBlock
          - AllowedWebBlock
          - WebserverPort
      - Label:
          default: Cluster Settings
        Parameters:
          - KeyPair
          - SchedulerInstanceType
          - WebserverInstanceType
          - WorkerInstanceType
          - WorkerConcurrency
          - MinGroupSize
          - MaxGroupSize
          - ShrinkThreshold
          - GrowthThreshold
          - DbMasterUsername
          - DbMasterPassword
      - Label:
          default: Airflow Config
        Parameters:
          - LoadExampleDags
          - LoadDefaultConns

Resources:

  # ----------------------- Security Groups START----------------------------

  Access:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for the shared filesystem across Airflow
        instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcBaseCidr
          IpProtocol: TCP
          FromPort: 2049
          ToPort: 2049
      VpcId: !Ref TargetVpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-access'

  Control:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node intercommunication between
        Airflow instances and remote access.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcBaseCidr
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref TargetVpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-control'

  Comms:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: >-
        Security Rules with permissions for node intercommunication between
        Airflow worker instances.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcBaseCidr
          IpProtocol: TCP
          FromPort: 8793
          ToPort: 8793
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref TargetVpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-comms'

  Web:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for the web UI exposed by Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref AllowedWebBlock
          IpProtocol: TCP
          FromPort: !Ref WebserverPort
          ToPort: !Ref WebserverPort
        - CidrIp: !Ref AllowedSshBlock
          IpProtocol: TCP
          FromPort: 22
          ToPort: 22
      VpcId: !Ref TargetVpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-web'

  Connection:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: Security Rules with permissions for database connections for Airflow.
      SecurityGroupIngress:
        - CidrIp: !Ref VpcBaseCidr
          IpProtocol: TCP
          FromPort: 5432
          ToPort: 5432
      VpcId: !Ref TargetVpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-connection'

  # ----------------------- Security Groups END------------------------------

#  # --------------------- Airflow Services START-----------------------------
#
#  AirflowScheduler:
#    Type: 'AWS::EC2::Instance'
#    Properties:
#      InstanceType: !Ref SchedulerInstanceType
#      ImageId: !FindInMap
#        - RegionToAMI
#        - !Ref 'AWS::Region'
#        - HVMGP2
#      SubnetId: !Ref StackSubnet
#      KeyName: !Ref KeyPair
#      SecurityGroupIds:
#        - !Ref Control
#      IamInstanceProfile: !Ref AirflowProfile
#      Tags:
#        - Key: Name
#          Value: !Sub '${ProjectNamePrefix}-scheduler'
#      UserData: !Base64
#        'Fn::Sub': |
#          #!/bin/bash -xe
#          echo $'TURBINE_MACHINE=SCHEDULER' > /etc/environment
#          /opt/aws/bin/cfn-init -v \
#            --region ${AWS::Region} \
#            --stack ${AWS::StackName} \
#            --resource Meta
#          /opt/aws/bin/cfn-signal -e $? \
#            --region ${AWS::Region} \
#            --stack ${AWS::StackName} \
#            --resource AirflowScheduler
#    DependsOn:
#      - EfsMountTarget
#      - Meta
#
#  AirflowWebserver:
#    Type: 'AWS::EC2::Instance'
#    Properties:
#      InstanceType: !Ref WebserverInstanceType
#      ImageId: !FindInMap
#        - RegionToAMI
#        - !Ref 'AWS::Region'
#        - HVMGP2
#      SubnetId: !Ref StackSubnet
#      KeyName: !Ref KeyPair
#      SecurityGroupIds:
#        - !Ref Web
#      IamInstanceProfile: !Ref AirflowProfile
#      Tags:
#        - Key: Name
#          Value: !Sub '${ProjectNamePrefix}-webserver'
#      UserData: !Base64
#        'Fn::Sub': |
#          #!/bin/bash -xe
#          echo $'TURBINE_MACHINE=WEBSERVER' > /etc/environment
#          /opt/aws/bin/cfn-init -v \
#            --region ${AWS::Region} \
#            --stack ${AWS::StackName} \
#            --resource Meta
#          /opt/aws/bin/cfn-signal -e $? \
#            --region ${AWS::Region} \
#            --stack ${AWS::StackName} \
#            --resource AirflowWebserver
#    DependsOn:
#      - EfsMountTarget
#      - Meta
#
#  Meta:
#    Type: 'AWS::CloudFormation::WaitConditionHandle'
#    Properties: {}
#    Metadata:
#      'AWS::CloudFormation::Init':
#        configSets:
#          default:
#            - filesys
#            - runtime
#            - service
#            - metahup
#            - cdagent
#        filesys:
#          # Mount EFS into EC2 on /mnt/efs
#          commands:
#            mkdir:
#              test: test ! -d /airflow
#              command: |
#                mkdir /airflow
#                chown -R ec2-user /airflow
#            mount:
#              test: test ! "$(mount | grep /mnt/efs)"
#              command: !Sub |
#                mkdir -p /mnt/efs
#                fspec="${EfsFileSystem}.efs.${AWS::Region}.amazonaws.com:/"
#                param="nfsvers=4.1,rsize=1048576,wsize=1048576"
#                param="$param,hard,timeo=600,retrans=2,noresvport"
#                echo "$fspec /mnt/efs nfs $param,_netdev 0 0" > /etc/fstab
#                mount /mnt/efs
#                chown -R ec2-user /mnt/efs
#        runtime:
#          # Install Airflow and all dependencies
#          packages:
#            yum:
#              git: []
#              gcc: []
#              gcc-c++: []
#              jq: []
#              lapack-devel: []
#              libcurl-devel: []
#              libxml2-devel: []
#              libxslt-devel: []
#              openssl-devel: []
#              postgresql-devel: []
#              python3: []
#              python3-devel: []
#              python3-pip: []
#              python3-wheel: []
#          commands:
#            install:
#              command: |
#                PYCURL_SSL_LIBRARY=openssl pip3 install \
#                  --no-cache-dir --compile --ignore-installed \
#                  pycurl
#                SLUGIFY_USES_TEXT_UNIDECODE=yes pip3 install \
#                  apache-airflow[celery,postgres,s3]==1.10.2 \
#                  celery[sqs] \
#                  billiard==3.5.0.4 \
#                  tenacity==4.12.0
#        service:
#          files:
#            /usr/bin/turbine:
#              mode: 755
#              content: |
#                #!/bin/sh
#                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
#                then exec airflow scheduler
#                elif [ "$TURBINE_MACHINE" == "WEBSERVER" ]
#                then exec airflow webserver
#                elif [ "$TURBINE_MACHINE" == "WORKER" ]
#                then exec airflow worker
#                else echo "TURBINE_MACHINE value unknown" && exit 1
#                fi
#            /etc/sysconfig/airflow:
#              content: !Sub
#                - |
#                  AWS_DEFAULT_REGION=${AWS::Region}
#                  AIRFLOW_HOME=/airflow
#                  AIRFLOW__CORE__EXECUTOR=CeleryExecutor
#                  AIRFLOW__CORE__LOAD_EXAMPLES=${LoadExampleDags}
#                  TURBINE__CORE__LOAD_DEFAULTS=${LoadDefaultConns}
#                  AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
#                  AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://${CfnLogsBucket}
#                  AIRFLOW__CORE__REMOTE_LOGGING=True
#                  AIRFLOW__WEBSERVER__BASE_URL=http://INJECTHOST:${WebserverPort}
#                  AIRFLOW__WEBSERVER__WEB_SERVER_PORT=${WebserverPort}
#                  AIRFLOW__CELERY__BROKER_URL=sqs://
#                  AIRFLOW__CELERY__DEFAULT_QUEUE=${queue}
#                  AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://${DbMasterUsername}:${DbMasterPassword}@${rds}/airflow
#                  AIRFLOW__CELERY__WORKER_CONCURRENCY=${WorkerConcurrency}
#                  AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__REGION=${AWS::Region}
#                - queue: !GetAtt
#                    - Tasks
#                    - QueueName
#                  rds: !GetAtt
#                    - Database
#                    - Endpoint.Address
#            /usr/lib/tmpfiles.d/airflow.conf:
#              content: |
#                D /run/airflow 0755 ec2-user ec2-user
#            /usr/lib/systemd/system/airflow.service:
#              content: |
#                [Service]
#                EnvironmentFile=/etc/sysconfig/airflow
#                User=ec2-user
#                Group=ec2-user
#                ExecStart=/usr/bin/turbine
#                Restart=always
#                RestartSec=5s
#                KillMode=mixed
#                TimeoutStopSec=24h
#                [Install]
#                WantedBy=multi-user.target
#            /usr/lib/systemd/system/watcher.path:
#              content: |
#                [Unit]
#                After=airflow.service
#                PartOf=airflow.service
#                [Path]
#                PathModified=/etc/sysconfig/airflow
#                [Install]
#                WantedBy=airflow.service
#            /usr/lib/systemd/system/watcher.service:
#              content: |
#                [Service]
#                Type=oneshot
#                ExecStartPre=/usr/bin/systemctl daemon-reload
#                ExecStart=/usr/bin/systemctl restart airflow
#            /usr/bin/lchkill:
#              mode: 755
#              content: !Sub |
#                #!/bin/sh
#                INSTANCE_ID=$(ec2-metadata -i | awk '{print $2}')
#                TERMINATE_MESSAGE="Terminating EC2 instance: $INSTANCE_ID"
#                TERMINATING=$(aws autoscaling describe-scaling-activities \
#                  --auto-scaling-group-name '${ProjectNamePrefix}-scaling-group' \
#                  --max-items 100 \
#                  --region '${AWS::Region}' | \
#                  jq --arg TERMINATE_MESSAGE "$TERMINATE_MESSAGE" \
#                  '.Activities[]
#                  | select(.Description
#                  | test($TERMINATE_MESSAGE)) != []')
#                if [ "$TERMINATING" = "true" ]
#                then
#                  systemctl stop airflow
#                fi
#            /usr/lib/systemd/system/lchkill.timer:
#              content: |
#                [Timer]
#                OnCalendar=*:0/1
#                [Install]
#                WantedBy=airflow.service
#            /usr/lib/systemd/system/lchkill.service:
#              content: |
#                [Service]
#                Type=oneshot
#                ExecStart=/usr/bin/lchkill
#            /usr/bin/lchbeat:
#              mode: 755
#              content: !Sub |
#                #!/bin/sh
#                SERVICE_STATUS=$(systemctl is-active airflow)
#                if [ "$SERVICE_STATUS" = "deactivating" ]
#                then
#                  aws autoscaling record-lifecycle-action-heartbeat \
#                    --instance-id $(ec2-metadata -i | awk '{print $2}') \
#                    --lifecycle-hook-name '${ProjectNamePrefix}-scaling-lfhook' \
#                    --auto-scaling-group-name '${ProjectNamePrefix}-scaling-group' \
#                    --region '${AWS::Region}'
#                fi
#            /usr/lib/systemd/system/lchbeat.timer:
#              content: |
#                [Timer]
#                OnCalendar=*:0/1
#                [Install]
#                WantedBy=airflow.service
#            /usr/lib/systemd/system/lchbeat.service:
#              content: |
#                [Service]
#                Type=oneshot
#                ExecStart=/usr/bin/lchbeat
#          commands:
#            setup:
#              command: !Sub |
#                cat /etc/environment >> /etc/sysconfig/airflow
#                PUBLIC=$(curl -s -o /dev/null -w "%{http_code}" \
#                  http://169.254.169.254/latest/meta-data/public-ipv4)
#                PUB_IPV4=$(ec2-metadata -v | awk '{print $2}')
#                LOC_IPV4=$(ec2-metadata -o | awk '{print $2}')
#                if [ $PUBLIC = "200" ]
#                then HOST=$PUB_IPV4
#                else HOST=$LOC_IPV4
#                fi
#                sed -i -e "s~INJECTHOST~$HOST~" /etc/sysconfig/airflow
#                sed 's/^/export /' -- </etc/sysconfig/airflow >/tmp/env.sh
#                source /tmp/env.sh
#                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
#                then if [ "$TURBINE__CORE__LOAD_DEFAULTS" == "True" ]
#                  then su -c '/usr/local/bin/airflow initdb' ec2-user
#                  else su -c '/usr/local/bin/airflow upgradedb' ec2-user
#                  fi
#                else echo "Database setup reserved for the scheduler"
#                fi
#                HAS_DEPLOYMENT=$(aws deploy list-deployments \
#                  --application-name ${AWS::StackName}-deployment-application \
#                  --deployment-group ${AWS::StackName}-deployment-group | \
#                  jq '.deployments | has(0)')
#                systemctl enable airflow.service watcher.path
#                if [ "$TURBINE_MACHINE" = "WORKER" ]
#                then systemctl enable lchkill.timer lchbeat.timer
#                fi
#                if [ "$TURBINE_MACHINE" = "WORKER" ] && \
#                   [ "$HAS_DEPLOYMENT" = "true" ]
#                then echo "Deployment pending, deferring service start"
#                else systemctl start airflow
#                fi
#        metahup:
#          files:
#            /etc/cfn/cfn-hup.conf:
#              content: !Sub |
#                [main]
#                stack=${AWS::StackId}
#                region=${AWS::Region}
#                interval=1
#              mode: '000400'
#              owner: root
#              group: root
#            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
#              content: !Sub |
#                [cfn-auto-reloader-hook]
#                triggers=post.update
#                path=Resources.Meta.Metadata.AWS::CloudFormation::Init
#                action=/opt/aws/bin/cfn-init -v \
#                  --region ${AWS::Region} \
#                  --stack ${AWS::StackName} \
#                  --resource Meta
#                runas=root
#            /lib/systemd/system/cfn-hup.service:
#              content: |
#                [Service]
#                ExecStart=/opt/aws/bin/cfn-hup
#                Restart=always
#                [Install]
#                WantedBy=multi-user.target
#          commands:
#            setup:
#              command: |
#                systemctl enable cfn-hup.service
#                systemctl start cfn-hup.service
#        # TODO: Explain CodeDeploy agent.. is here the CodeDeploy package being fetched?
#        # Explain address https://aws-codedeploy-${AWS::Region}.s3.amazonaws.com/latest/install
#        cdagent:
#          packages:
#            yum:
#              ruby: []
#              wget: []
#          commands:
#            install:
#              command: !Sub |
#                wget https://aws-codedeploy-${AWS::Region}.s3.amazonaws.com/latest/install
#                chmod +x ./install
#                ./install auto
#
#  AirflowWorkerConfig:
#    Type: 'AWS::AutoScaling::LaunchConfiguration'
#    Properties:
#      ImageId: !FindInMap
#        - RegionToAMI
#        - !Ref 'AWS::Region'
#        - HVMGP2
#      InstanceType: !Ref WorkerInstanceType
#      KeyName: !Ref KeyPair
#      SecurityGroups:
#        - !Ref Comms
#      IamInstanceProfile: !Ref AirflowProfile
#      UserData: !Base64
#        'Fn::Sub': |
#          #!/bin/bash -xe
#          echo $'TURBINE_MACHINE=WORKER' > /etc/environment
#          /opt/aws/bin/cfn-init -v \
#            --region ${AWS::Region} \
#            --stack ${AWS::StackName} \
#            --resource Meta
#    DependsOn:
#      - Meta
#
#  AirflowRole:
#    Type: 'AWS::IAM::Role'
#    Properties:
#      AssumeRolePolicyDocument:
#        Version: 2012-10-17
#        Statement:
#          - Effect: Allow
#            Principal:
#              Service:
#                - ec2.amazonaws.com
#            Action:
#              - 'sts:AssumeRole'
#      Policies:
#        - PolicyName: !Sub '${ProjectNamePrefix}-queue-rw-policy'
#          PolicyDocument:
#            Version: 2012-10-17
#            Statement:
#              - Effect: Allow
#                Action:
#                  - 'sqs:ListQueues'
#                Resource:
#                  - !Sub 'arn:aws:sqs:*:${AWS::AccountId}:*'
#              - Effect: Allow
#                Action:
#                  - 'sqs:ChangeMessageVisibility'
#                  - 'sqs:DeleteMessage'
#                  - 'sqs:GetQueueAttributes'
#                  - 'sqs:GetQueueUrl'
#                  - 'sqs:ReceiveMessage'
#                  - 'sqs:SendMessage'
#                Resource: !Sub
#                  - 'arn:aws:sqs:*:${AWS::AccountId}:${queue}'
#                  - queue: !GetAtt
#                      - Tasks
#                      - QueueName
#        - PolicyName: !Sub '${ProjectNamePrefix}-deployments-r-policy'
#          PolicyDocument:
#            Version: 2012-10-17
#            Statement:
#              - Effect: Allow
#                Action:
#                  - 's3:Get*'
#                  - 's3:List*'
#                Resource: !Sub 'arn:aws:s3:::${DeploymentsBucket}/*'
#              - Effect: Allow
#                Action:
#                  - 'codedeploy:List*'
#                Resource: !Sub 'arn:aws:codedeploy:*:${AWS::AccountId}:deploymentgroup:*'
#        - PolicyName: !Sub '${ProjectNamePrefix}-logs-rw-policy'
#          PolicyDocument:
#            Version: 2012-10-17
#            Statement:
#              - Effect: Allow
#                Action:
#                  - 's3:Get*'
#                  - 's3:Put*'
#                Resource: !Sub 'arn:aws:s3:::${CfnLogsBucket}/*'
#        - PolicyName: !Sub '${ProjectNamePrefix}-lifecycle-heartbeat'
#          PolicyDocument:
#            Version: 2012-10-17
#            Statement:
#              - Effect: Allow
#                Action:
#                  - 'autoscaling:RecordLifecycleActionHeartbeat'
#                  - 'autoscaling:CompleteLifecycleAction'
#                Resource: !Sub 'arn:aws:autoscaling:*:${AWS::AccountId}:autoScalingGroup:*:*'
#              - Effect: Allow
#                Action:
#                  - 'autoscaling:DescribeScalingActivities'
#                Resource: '*'
#
#  AirflowProfile:
#    Type: 'AWS::IAM::InstanceProfile'
#    Properties:
#      Roles:
#        - !Ref AirflowRole
#
#  # --------------------------- Shared Storage-------------------------------------
#
#  EfsFileSystem:
#    Type: 'AWS::EFS::FileSystem'
#    Properties:
#      FileSystemTags:
#        - Key: Name
#          Value: !Sub '${ProjectNamePrefix}-filesystem'
#
#  EfsMountTarget:
#    Type: 'AWS::EFS::MountTarget'
#    Properties:
#      FileSystemId: !Ref EfsFileSystem
#      SubnetId: !Ref StackSubnet
#      SecurityGroups:
#        - !Ref Access
#
#  # --------------------------- Logs----------------------------------------------
#
##  LogsBucket:
##      Type: 'AWS::S3::Bucket'
##      Properties:
##        BucketName: !Sub '${ProjectNamePrefix}-logs'
#
#  Logger:
#    Type: 'AWS::IAM::Role'
#    Properties:
#      AssumeRolePolicyDocument:
#        Version: 2012-10-17
#        Statement:
#          - Effect: Allow
#            Principal:
#              Service: lambda.amazonaws.com
#            Action: 'sts:AssumeRole'
#      Policies:
#        - PolicyName: !Sub '${ProjectNamePrefix}-cloudwatch-policy'
#          PolicyDocument:
#            Version: 2012-10-17
#            Statement:
#              - Effect: Allow
#                Resource: '*'
#                Action:
#                  - 'cloudwatch:GetMetric*'
#                  - 'cloudwatch:PutMetricData'
#
#  # ----------------------- SQS---------------------------------------------------
#
#  Tasks:
#    Type: 'AWS::SQS::Queue'
#    Properties: {}
#
#  # ----------------------- Airflow Services END----------------------------------
#
#  # ----------------------- Code Deploy START-------------------------------------
#
#  DeploymentsBucket:
#    Type: 'AWS::S3::Bucket'
#    Properties:
#      BucketName: !Ref CodeDeployBucket
#
#  CodeDeployApplication:
#    Type: 'AWS::CodeDeploy::Application'
#    Properties:
#      ApplicationName: !Sub '${ProjectNamePrefix}-deployment-application'
#      ComputePlatform: Server
#
#  CodeDeployDeploymentGroup:
#    Type: 'AWS::CodeDeploy::DeploymentGroup'
#    Properties:
#      ApplicationName: !Ref CodeDeployApplication
#      DeploymentGroupName: !Sub '${ProjectNamePrefix}-deployment-group'
#      AutoScalingGroups:
#        - !Ref AutoScalingGroup
#      Ec2TagSet:
#        Ec2TagSetList:
#          - Ec2TagGroup:
#              - Type: KEY_AND_VALUE
#                Key: Name
#                Value: !Sub '${ProjectNamePrefix}-scheduler'
#              - Type: KEY_AND_VALUE
#                Key: Name
#                Value: !Sub '${ProjectNamePrefix}-webserver'
#      ServiceRoleArn: !GetAtt
#        - CodeDeployServiceRole
#        - Arn
#
#  CodeDeployServiceRole:
#    Type: 'AWS::IAM::Role'
#    Properties:
#      AssumeRolePolicyDocument:
#        Version: 2012-10-17
#        Statement:
#          - Effect: Allow
#            Principal:
#              Service:
#                - codedeploy.amazonaws.com
#            Action:
#              - 'sts:AssumeRole'
#      ManagedPolicyArns:
#        - 'arn:aws:iam::aws:policy/service-role/AWSCodeDeployRole'
#
#  # ----------------------- Code Deploy END-------------------------------------
#
#  # ----------------------- Scaling Mechanism START-----------------------------
#
#  AutoScalingGroup:
#    Type: 'AWS::AutoScaling::AutoScalingGroup'
#    Properties:
#      AutoScalingGroupName: !Sub '${ProjectNamePrefix}-scaling-group'
#      LaunchConfigurationName: !Ref AirflowWorkerConfig
#      MinSize: !Ref MinGroupSize
#      MaxSize: !Ref MaxGroupSize
#      MetricsCollection:
#        - Granularity: 1Minute
#      VPCZoneIdentifier:
#        - !Ref StackSubnet
#      Tags:
#        - Key: Name
#          Value: !Sub '${ProjectNamePrefix}-worker'
#          PropagateAtLaunch: true
#
#  AutoScalingHook:
#    Type: 'AWS::AutoScaling::LifecycleHook'
#    Properties:
#      AutoScalingGroupName: !Ref AutoScalingGroup
#      DefaultResult: CONTINUE
#      HeartbeatTimeout: 300
#      LifecycleHookName: !Sub '${ProjectNamePrefix}-scaling-lfhook'
#      LifecycleTransition: 'autoscaling:EC2_INSTANCE_TERMINATING'
#
#  Metric:
#    Type: 'AWS::Lambda::Function'
#    Properties:
#      Runtime: nodejs8.10
#      Handler: index.handler
#      Code:
#        ZipFile: !Sub
#          - |
#            var AWS = require('aws-sdk');
#            AWS.config.update({region: '${AWS::Region}'});
#            var cw = new AWS.CloudWatch({apiVersion: '2010-08-01'});
#            const datePlusMinutes = (d, m) => {
#              const _d = new Date(d);
#              _d.setMinutes(d.getMinutes() + m);
#              return _d;
#            };
#            const getMetricAtTime = (ms, m, t) => {
#              const m_idx = ms.MetricDataResults
#                .map(_r => _r.Id)
#                .indexOf(m);
#              const t_idx = ms.MetricDataResults[m_idx]
#                .Timestamps
#                .map(_t => _t.toISOString())
#                .indexOf(t.toISOString());
#              return ms.MetricDataResults[m_idx]
#                .Values[t_idx];
#            };
#            const discount = (ms, m, t1, t2, ws) => {
#              let incs = 0, d = t1;
#              let v1 = getMetricAtTime(ms, m, d), v2;
#              for (let i = 0; d < t2 ; i++) {
#                d = datePlusMinutes(t1, i+1);
#                v2 = getMetricAtTime(ms, m, d);
#                if (v2 > v1) incs += ws[i];
#                v1 = v2;
#              }
#              return incs;
#            };
#            exports.handler = async function(event, context) {
#              let curr = new Date();
#              curr.setMinutes(Math.floor(curr.getMinutes()/5)*5-5);
#              curr.setSeconds(0); curr.setMilliseconds(0);
#              const prev = datePlusMinutes(curr, -5);
#              const back = datePlusMinutes(prev, -5);
#              const metrics = await cw.getMetricData({
#                StartTime: back, EndTime: curr,
#                ScanBy: 'TimestampDescending',
#                MetricDataQueries: [
#                  { Id: 'maxANOMV', MetricStat: {
#                      Metric: { Namespace: 'AWS/SQS',
#                                MetricName: 'ApproximateNumberOfMessagesVisible',
#                                Dimensions: [{ Name: 'QueueName',
#                                              Value: '${queueName}' }]},
#                      Period: 300,
#                      Stat: 'Maximum',
#                      Unit: 'Count' }},
#                  { Id: 'sumNOER', MetricStat: {
#                      Metric: { Namespace: 'AWS/SQS',
#                                MetricName: 'NumberOfEmptyReceives',
#                                Dimensions: [{ Name: 'QueueName',
#                                              Value: '${queueName}' }]},
#                      Period: 300,
#                      Stat: 'Sum',
#                      Unit: 'Count', }},
#                  { Id: 'avgGISI', MetricStat: {
#                      Metric: { Namespace: 'AWS/AutoScaling',
#                                MetricName: 'GroupInServiceInstances',
#                                Dimensions: [{ Name: 'AutoScalingGroupName',
#                                              Value: '${asgName}' }]},
#                      Period: 300,
#                      Stat: 'Average',
#                      Unit: 'None', }},
#                  { Id: 'uGISI', MetricStat: {
#                      Metric: { Namespace: 'AWS/AutoScaling',
#                                MetricName: 'GroupDesiredCapacity',
#                                Dimensions: [{ Name: 'AutoScalingGroupName',
#                                              Value: '${asgName}' }]},
#                      Period: 60,
#                      Stat: 'Average',
#                      Unit: 'None', }},
#              ]}).promise();
#              const ANOMV = getMetricAtTime(metrics, 'maxANOMV', prev);
#              const NOER = getMetricAtTime(metrics, 'sumNOER', prev);
#              const GISI = getMetricAtTime(metrics, 'avgGISI', prev);
#              const ws = [0, 0, 0, 0.1, 0.3, 0.3, 0.3, 0.3, 0.2];
#              const dGISI = discount(metrics, 'uGISI', back, curr, ws);
#              const M = GISI - dGISI;
#              let l;
#              if (M > 0)
#                l = 1 - NOER / (M * 0.098444 * 300);
#              else
#                l = (ANOMV > 0) ? 1.0 : 0.0;
#              await cw.putMetricData({
#                Namespace: 'Turbine',
#                MetricData: [{ MetricName: 'WorkerLoad',
#                              Dimensions: [ { Name: 'StackName',
#                                              Value: '${AWS::StackName}' }],
#                              Timestamp: prev,
#                              Value: (l > 0) ? l : 0,
#                              Unit: 'None' }],
#              }).promise();
#            };
#          - asgName: !Sub '${ProjectNamePrefix}-scaling-group'
#            queueName: !GetAtt
#              - Tasks
#              - QueueName
#      Role: !GetAtt
#        - Logger
#        - Arn
#
#  Timer:
#    Type: 'AWS::Events::Rule'
#    Properties:
#      ScheduleExpression: rate(1 minute)
#      State: ENABLED
#      Targets:
#        - Arn: !GetAtt
#            - Metric
#            - Arn
#          Id: TargetFunction
#
#  Invoke:
#    Type: 'AWS::Lambda::Permission'
#    Properties:
#      FunctionName: !Ref Metric
#      Action: 'lambda:InvokeFunction'
#      Principal: events.amazonaws.com
#      SourceArn: !GetAtt
#        - Timer
#        - Arn
#
#  Shrink:
#    Type: 'AWS::CloudWatch::Alarm'
#    Properties:
#      AlarmActions:
#        - !Ref ScaleIn
#      Namespace: Turbine
#      MetricName: WorkerLoad
#      Dimensions:
#        - Name: StackName
#          Value: !Ref 'AWS::StackName'
#      Statistic: Average
#      Period: 300
#      EvaluationPeriods: 1
#      Threshold: !Ref ShrinkThreshold
#      ComparisonOperator: LessThanOrEqualToThreshold
#
#  Growth:
#    Type: 'AWS::CloudWatch::Alarm'
#    Properties:
#      AlarmActions:
#        - !Ref ScaleOut
#      Namespace: Turbine
#      MetricName: WorkerLoad
#      Dimensions:
#        - Name: StackName
#          Value: !Ref 'AWS::StackName'
#      Statistic: Average
#      Period: 300
#      EvaluationPeriods: 1
#      Threshold: !Ref GrowthThreshold
#      ComparisonOperator: MoreThanOrEqualToThreshold
#
#  ScaleIn:
#    Type: 'AWS::AutoScaling::ScalingPolicy'
#    Properties:
#      AdjustmentType: ChangeInCapacity
#      PolicyType: SimpleScaling
#      ScalingAdjustment: -1
#      Cooldown: '600'
#      AutoScalingGroupName: !Ref AutoScalingGroup
#
#  ScaleOut:
#    Type: 'AWS::AutoScaling::ScalingPolicy'
#    Properties:
#      AdjustmentType: ChangeInCapacity
#      PolicyType: SimpleScaling
#      ScalingAdjustment: 1
#      Cooldown: '600'
#      AutoScalingGroupName: !Ref AutoScalingGroup
#
#  # ----------------------- Scaling Mechanism END-----------------------------
#
#  # ------------------------- Database START-------------------------------------
#
#  DBs:
#    Type: 'AWS::RDS::DBSubnetGroup'
#    Properties:
#      DBSubnetGroupDescription: Associates the Database Instances with the selected VPC Subnets.
#      SubnetIds:
#        - !Ref StackSubnet
#        - !Ref DummySubnet
#
#  Database:
#    Type: 'AWS::RDS::DBInstance'
#    Properties:
#      AllocatedStorage: '20'
#      DBInstanceClass: db.t2.micro
#      DBName: airflow
#      Engine: postgres
#      MasterUsername: !Ref DbMasterUsername
#      MasterUserPassword: !Ref DbMasterPassword
#      Tags:
#        - Key: Name
#          Value: !Sub '${AWS::StackName}-database'
#      DBSubnetGroupName: !Ref DBs
#      VPCSecurityGroups:
#        - !Ref Connection
#
#   # ------------------------- Database END-------------------------------------
#
